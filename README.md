# ğŸ“¦ DevOps EKS Portfolio: Automated Deployment with Argo CD
A cloud-native DevOps portfolio project using GitOps, EKS, and CI/CD automation
---

## ğŸ“ Repository Structure

```bash
â”œâ”€â”€ .github/workflows/
â”‚   â”œâ”€â”€ deploy-eks-argocd.yaml        # GitHub Actions pipeline to deploy EKS, Argo CD, and apps
â”‚   â””â”€â”€ destroy-eks-argocd.yaml       # GitHub Actions pipeline to destroy the EKS cluster and associated resources
â”œâ”€â”€ terraform/                        # Contains all Terraform configurations
â”‚   â”œâ”€â”€ main.tf                       # VPC, EKS cluster, and IAM roles definition
â”‚   â”œâ”€â”€ outputs.tf                    # Terraform output values (e.g., cluster endpoint)
â”‚   â”œâ”€â”€ provider.tf                   # AWS provider & backend configuration
â”‚   â””â”€â”€ variables.tf                  # Input variables (optional if hardcoded)
â”œâ”€â”€ k8s/                              # Kubernetes manifests and Helm charts
â”‚   â”œâ”€â”€ 2048-app.yaml                 # ArgoCD Application CR for 2048 game
â”‚   â”œâ”€â”€ 2048-game/                    # Helm chart for deploying the 2048 game
â”‚   â”‚   â””â”€â”€ 2048-game/
â”‚   â”‚       â”œâ”€â”€ Chart.yaml            # Helm chart metadata
â”‚   â”‚       â”œâ”€â”€ templates/            # Kubernetes templates for 2048 game
â”‚   â”‚       â”‚   â”œâ”€â”€ _helpers.tpl
â”‚   â”‚       â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”‚       â”‚   â”œâ”€â”€ hpa.yaml
â”‚   â”‚       â”‚   â”œâ”€â”€ ingress.yaml
â”‚   â”‚       â”‚   â”œâ”€â”€ NOTES.txt
â”‚   â”‚       â”‚   â”œâ”€â”€ service.yaml
â”‚   â”‚       â”‚   â”œâ”€â”€ serviceaccount.yaml
â”‚   â”‚       â”‚   â””â”€â”€ tests/
â”‚   â”‚       â”‚       â””â”€â”€ test-connection.yaml
â”‚   â”‚       â””â”€â”€ values.yaml           # Default Helm chart values
â”‚   â”œâ”€â”€ argocd/
â”‚   â”‚   â”œâ”€â”€ bootstrap-apps.yaml       # ArgoCD ApplicationSet for GitOps apps
â”‚   â”‚   â””â”€â”€ install.yaml              # Argo CD installation manifest
â”‚   â”œâ”€â”€ grafana-app.yaml              # ArgoCD Application for Grafana
â”‚   â””â”€â”€ prometheus-app.yaml           # ArgoCD Application for Prometheus
â”œâ”€â”€ knownissues.md                    # Known setup errors and troubleshooting guide
â”œâ”€â”€ README.md                         # Primary documentation for deploying the full stack
```

---

## âœ… Pre-requisites

* An AWS Account
* GitHub Account
* Permissions to create EKS, VPC, IAM Roles, and S3 bucket in AWS
* GitHub secrets set:

  * `TF_BACKEND_BUCKET` â€” S3 bucket name for remote state
  * `TF_BACKEND_REGION` â€” Region of your deployment (e.g., `us-east-1`)
  * `GITHUB_ROLE_ARN` â€” IAM Role ARN with GitHub OIDC trust policy and admin permissions

> âš ï¸ Important: Ensure the IAM Role created in AWS has `enable_cluster_creator_admin_permissions = true` in your Terraform EKS module.

---

## ğŸš€ How to Use This Repository

### Step 1: Fork This Repository

1. Click on `Fork` in the top right corner of GitHub
2. Clone your fork locally: `git clone https://github.com/<your-username>/devops-eks-portfolio.git`

### Step 2: Setup AWS Role for GitHub OIDC (One-time setup)

1. Go to **IAM > Roles** and click **Create role**
2. Select **Web Identity**
3. Choose **GitHub** as the provider
4. Set your **GitHub Repo or Org name** (e.g., `your_username/devops-eks-portfolio`)
5. Add required IAM policies (e.g., AdministratorAccess or scoped EKS/VPC permissions)
6. Save the **Role ARN**, and store it as `GITHUB_ROLE_ARN` in your GitHub repo secrets

### Step 3: Setup GitHub Secrets

In your repo, go to **Settings > Secrets and Variables > Actions** and set the following:

| Secret Name         | Description                         |
| ------------------- | ----------------------------------- |
| `AWS_REGION`        | AWS Region (e.g., `us-east-1`)      |
| `TF_BACKEND_REGION` | Same as above                       |
| `TF_BACKEND_BUCKET` | S3 bucket name you created manually |
| `GITHUB_ROLE_ARN`   | ARN of your GitHub OIDC IAM Role    |

### Step 4: Trigger Deployment

Commit any changes to the `main` branch or push it â€” GitHub Actions will:

1. Deploy EKS and VPC via Terraform
2. Install Argo CD
3. Deploy sample apps (Grafana, 2048 game) using Argo CD GitOps
4. Output public endpoints and Argo CD admin password
> âš ï¸ Important: Region name will be masked in the endpoints generated. Replace it with the region where your resources are deployed for eg. replace the url(https://adb0787fcc514455e95d95a6e53.***.elb.amazonaws.com) with region name us-east-1 as "https://adb0787fcc514455e95d95a6e53.us-east-1.elb.amazonaws.com"

---

## ğŸ” Argo CD Access

Once the pipeline completes, GitHub Actions will automatically print the default Argo CD admin password in the logs.

### Access Argo CD UI:

* Get the public endpoint from the CI outputs or 
* From your local terminal using the below command once it is successfully configured using the test and validate section below:

  ```bash
  kubectl get svc argocd-server -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'
  ```
* Open in browser: `https://<loadbalancer-hostname>`
* Login with username `admin` and the password retrieved from the pipeline output

---

## ğŸ§© Applications Deployed

* **Argo CD** (Namespace: `argocd`)
* **Grafana** (Namespace: `monitoring`)
* **2048 Game** (Namespace: `default`)

---

## ğŸ§ª Testing and Validation

After deployment, test access locally:

### Step 1: Configure AWS CLI

```bash
aws configure
```

### Step 2: Update kubeconfig for kubectl

```bash
aws eks update-kubeconfig --region us-east-1 --name devops-eks-portfolio-cluster
```

### Step 3: Validate Kubernetes Access

```bash
kubectl get nodes
kubectl get pods -A
kubectl get svc -A | grep LoadBalancer
```

You should see external LoadBalancer hostnames for:

* ArgoCD
* Grafana
* 2048 Game

### Step 4: Retrieve Argo CD Password (Alternative)

```bash
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d && echo
```

---

## ğŸ§¹ Resource Cleanup

To destroy all infrastructure and cleanup AWS resources created by Terraform:

1. Navigate to the GitHub Actions tab
2. Run the `destroy-eks-argocd.yaml` workflow manually or trigger it via branch/tag

This workflow will:

* Destroy all Terraform-managed resources (EKS, VPC, IAM roles, etc.)
* Clean up any related Kubernetes objects

---

## ğŸ› ï¸ Additional Automation

The following automation is already configured in the GitHub Actions workflow:

* Argo CD server exposed via LoadBalancer
* Namespace creation and app deployment
* Dynamic waiting for LoadBalancer IPs
* Auto-retrieval of Argo CD admin password (prints to pipeline logs)
* Display of ArgoCD, Grafana, and 2048 Game public endpoints

---

## ğŸ“Œ Notes

* Cluster will fail if `enable_cluster_creator_admin_permissions = false`
* Terraform stores state in S3 â€” ensure S3 bucket exists before running the pipeline

---

## ğŸ“„ Licensing

MIT License â€” Free to use, share, modify.

---

For advanced troubleshooting and known issues, refer to [`KNOWN_ISSUES.md`](./KNOWN_ISSUES.md)

---

Happy DevOps-ing! ğŸš€
